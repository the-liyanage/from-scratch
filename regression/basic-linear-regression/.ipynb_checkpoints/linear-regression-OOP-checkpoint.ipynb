{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c76ee19f-dab9-40ea-b537-768ea41c0959",
   "metadata": {},
   "source": [
    "## Simple Linear Regression From Scratch \n",
    "\n",
    "#### Goal: Find the best straight line through data points\n",
    "#### Formula: y = wx + b\n",
    "\n",
    "- Where:\n",
    "  - w = slope (how steep the line is)\n",
    "  - b = y - intercept (where line crosses y-axis)\n",
    "  - x = input (independent variable)\n",
    "  - y = output (dependent variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895e0fc6-94a3-463e-9c8f-778c471cd9e3",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "### OOP Concepts Covered:\n",
    "1. Class - Blueprint for creating regression models\n",
    "2. Constructor `(__init__)` - Initialize model when created\n",
    "3. Instance Variables (self.w, self.b) - Data stored in each model\n",
    "4. Methods (fit, predict, evaluate) - Actions the model can perform\n",
    "5. Encapsulation - Bundling data and methods together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e736c4b-eb23-4f28-8d2c-b33a780048ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21cb34fc-50c6-46f5-8f41-7858a6101008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Synthetic data...\n",
      "\n",
      "Generated 100 data points\n",
      " x=0.64 -> y =5.96\n",
      " x=5.10 -> y =16.99\n",
      " x=5.38 -> y =14.87\n",
      " x=6.73 -> y =18.35\n",
      " x=7.89 -> y =24.83\n",
      " x=8.88 -> y =27.45\n",
      " x=3.03 -> y =12.56\n",
      " x=8.42 -> y =25.42\n",
      " x=7.02 -> y =19.24\n",
      " x=5.14 -> y =17.08\n",
      "Training set: 80 samples\n",
      "Test set: 20 samples\n",
      "Learning rate: 0.01\n",
      "Iterations: 1000\n",
      "Initial Slope(w): 0.0\n",
      "Intial Intercept(b): 0.0\n",
      "\n",
      "Number of training examples: 80\n",
      "\n",
      "Training completed\n",
      "Final Slope (w):\n",
      "2.57\n",
      "Final Intercept (b):\n",
      "2.71\n",
      "Final Cost:\n",
      "2.1131\n",
      "Learned Equation: y = 2.5730x + 2.7097\n",
      "\n",
      "First 5 test predictions vs actual:\n",
      " x=3.77 -> Predicted: 12.42,  Actual: 12.13, Error: 0.29\n",
      " x=7.71 -> Predicted: 22.56,  Actual: 22.06, Error: 0.50\n",
      " x=2.99 -> Predicted: 10.40,  Actual: 10.36, Error: 0.03\n",
      " x=4.89 -> Predicted: 15.28,  Actual: 17.42, Error: 2.14\n",
      " x=2.73 -> Predicted: 9.74,  Actual: 8.34, Error: 1.40\n"
     ]
    }
   ],
   "source": [
    "class SimpleLinearRegression():\n",
    "    \"\"\"\n",
    "    This class represents ONE linear regression model.\n",
    "    Each model learns its own slope(w) and intercept(b).\n",
    "    \"\"\"\n",
    "\n",
    "    #CONSTRUCTOR: Initialize a new model\n",
    "    def __init__(self, learning_rate = 0.01, iterations = 1000):\n",
    "        \"\"\"\n",
    "        1. python creates a new object in memeory.\n",
    "        2. __init__ is called automatically.\n",
    "        3. self.lr gets set to 0.01\n",
    "        4. self.iterations gets set to 1000\n",
    "        5. Other attributes initialized to staring values.\n",
    "        \"\"\"\n",
    "\n",
    "        #Instance Variable (Attributes)\n",
    "        #These belong to THIS specific model instance.\n",
    "        #Different models can have different values.\n",
    "\n",
    "        #Store hyperparameters (settings we choose)\n",
    "        self.lr = learning_rate\n",
    "        self.iterations = iterations\n",
    "\n",
    "        #Model parameters (What the model LEARNS)\n",
    "        self.w = 0.0\n",
    "        self.b = 0.0\n",
    "\n",
    "        #Training history (for visualization) \n",
    "        self.cost_history = [] #Track cost at each iteration\n",
    "\n",
    "        #Status Flag\n",
    "        self.is_trained = False  #Has the model been trained yet?\n",
    "\n",
    "\n",
    "        print(f\"Learning rate: {self.lr}\")\n",
    "        print(f\"Iterations: {self.iterations}\")\n",
    "        print(f\"Initial Slope(w): {self.w}\")\n",
    "        print(f\"Intial Intercept(b): {self.b}\")\n",
    "        print()\n",
    "\n",
    "    #METHOD: fit() -----> Train the model (learn m and b)\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model to find the best w and b\n",
    "\n",
    "        Parameters:\n",
    "        X : list\n",
    "            Input Values (Independent variables)\n",
    "            eg: [1, 2, 3, 4, 5]\n",
    "\n",
    "        y: list\n",
    "            Output Values (dependent variables)\n",
    "            eg:[90, 80, 30, 40,  20]\n",
    "\n",
    "        Gradient Descent Steps:\n",
    "        1. Start with random/zero values for w and b\n",
    "        2. Make predictions with current w and b\n",
    "        3. Calculate how wrong we are (cost function)\n",
    "        4. Calculate gradients (which direction to adjust w and b)\n",
    "        5. Update w and b in that direction\n",
    "        6. Repeat steps many times\n",
    "        7. Eventually converge to best w and b\n",
    "        \"\"\"\n",
    "\n",
    "        #Number of training examples\n",
    "        n = len(X)\n",
    "        print(f\"Number of training examples: {n}\\n\")\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "        #Gradient Descent loop\n",
    "        for iterations in range(self.iterations):\n",
    "            predictions = []\n",
    "            for i in range(n):\n",
    "                y_pred = self.w * X[i] + self.b\n",
    "                predictions.append(y_pred)\n",
    "\n",
    "\n",
    "            #Calculate Cost (Mean Squared Error)\n",
    "            #Cost Measures how bad our predictions are\n",
    "            #Lower cost --> better predictions\n",
    "            #Formula: Cost = (1/n) *  Σ(y_pred - y_actual)²\n",
    "\n",
    "            total_error = 0.0\n",
    "            for i in range(n):\n",
    "                error = predictions[i] - y[i]\n",
    "                squared_error = error ** 2\n",
    "                total_error += squared_error\n",
    "            cost = total_error /n \n",
    "\n",
    "            self.cost_history.append(cost)\n",
    "\n",
    "\n",
    "            #Calculate Gradients \n",
    "            #Gradients tell us HOW to change w and b to reduce cost\n",
    "            # These formulas come from calculus (partial derivatives):\n",
    "            # ∂Cost/∂m = (2/n) * Σ(y_pred - y_actual) * x\n",
    "            # ∂Cost/∂b = (2/n) * Σ(y_pred - y_actual)\n",
    "\n",
    "            #Gradient for w (slope)\n",
    "            gradient_w = 0.0\n",
    "            for i in range(n):\n",
    "                error = predictions[i] - y[i]\n",
    "                gradient_w += error * X[i]\n",
    "            gradient_w = (2/n) * gradient_w\n",
    "\n",
    "            gradient_b = 0.0\n",
    "            for i in range(n):\n",
    "                error = predictions[i] - y[i]\n",
    "                gradient_b += error\n",
    "            gradient_b = (2/n) * gradient_b\n",
    "\n",
    "\n",
    "            #Update Parameters\n",
    "            self.w = self.w - self.lr * gradient_w\n",
    "            self.b = self.b - self.lr * gradient_b\n",
    "\n",
    "            #Print every 100 iterations \n",
    "            if(self.iterations + 1 )% 100 == 0 or self.iterations == 0 or self.iterations == self.iterations -1:\n",
    "                print(f\"{iteration+1:<6} {cost:<15.2f} {self.w:<15.2f} {self.b:<15.2f}\")\n",
    "\n",
    "        self.is_trained = True\n",
    "        print(\"Training completed\")\n",
    "        print(f\"Final Slope (w):\\n{self.w:.2f}\")\n",
    "        print(f\"Final Intercept (b):\\n{self.b:.2f}\")\n",
    "        print(f\"Final Cost:\\n{cost:.4f}\")\n",
    "        print(f\"Learned Equation: y = {self.w:.4f}x + {self.b:.4f}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "    #METHOD: predict()\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the learned equation: y = wx + b\n",
    "        Parameters:\n",
    "        \n",
    "        X: list or single value\n",
    "           Input value(s) to make predictions\n",
    "\n",
    "        Returns:\n",
    "        list or float : predicted y value(s)\n",
    "\n",
    "        eg:\n",
    "        model.predict([6, 7, 8]) ---> Predict for multiple values\n",
    "        model.predict(6) ----> Predict for single value\n",
    "\n",
    "        How it works:\n",
    "        1. Check if model is trained. (can't predict without w and b)\n",
    "        2. For each x, calculate y = wx + b\n",
    "        3. Return predictions\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.is_trained:\n",
    "            raise Exception(\"ERROR: Model must be trained before making predictions\")\n",
    "\n",
    "        #Handle single value Vs a list\n",
    "        if isinstance(X, (int, float)):\n",
    "            #Single value --> return single predictions\n",
    "            return self.w * X + self.b\n",
    "        else:\n",
    "            #List of values --> return list of predictions\n",
    "            predictions = []\n",
    "            for x in X:\n",
    "                y_pred = self.w * x + self.b\n",
    "                predictions.append(y_pred)\n",
    "            return predictions\n",
    "\n",
    "    #METHOD: Get model parameters\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Get the learned parameters.\n",
    "\n",
    "        Returns:\n",
    "        dict : {'w': slope, 'b': intercept}\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'w':self.w,\n",
    "            'b':self.b\n",
    "\n",
    "        }\n",
    "            \n",
    "#Demonstration function\n",
    "def generate_linear_data(n_samples = 50, w_true=2.5, b_true = 3.0, noise = 5.0):\n",
    "    \"\"\"\n",
    "    Generate synthetic data with a linear relationship.\n",
    "\n",
    "    Formula: y = w_true * x + b_true + noise\n",
    "\n",
    "    Parameters:\n",
    "    n_samples: int\n",
    "        - Number of data points to generate\n",
    "    w_true: float \n",
    "        - True slope (what we want the model to learn)\n",
    "    b_true: float\n",
    "        - True intercept\n",
    "    noise: float\n",
    "        - Amount of random noise to add (make it realistic)\n",
    "\n",
    "    Returns :\n",
    "    X, y L lists of inputs and outputs\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        x = random.random() * 10\n",
    "        y_true = w_true * x + b_true\n",
    "\n",
    "        #Add random noise \n",
    "        noise_value = (random.random()- 0.5) * noise\n",
    "        y_noisy = y_true + noise_value\n",
    "        X.append(x)\n",
    "        y.append(y_noisy)\n",
    "    return X, y\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function demostrating the complete worlflow\n",
    "\n",
    "    \"\"\"\n",
    "    #Generate synthetic data\n",
    "    print(\"Generating Synthetic data...\")\n",
    "    print()\n",
    "\n",
    "    X, y = generate_linear_data(\n",
    "        n_samples = 100,\n",
    "        w_true = 2.5,\n",
    "        b_true = 3.0,\n",
    "        noise = 5.0\n",
    "\n",
    "    )\n",
    "\n",
    "    print(f\"Generated {len(X)} data points\")\n",
    "    for i in range(10):\n",
    "        print(f\" x={X[i]:.2f} -> y ={y[i]:.2f}\")\n",
    "\n",
    "\n",
    "    #Split into train and test-sets\n",
    "    \n",
    "    split_idx = int(0.8 * len(X))\n",
    "    X_train = X[:split_idx]\n",
    "    y_train = y[:split_idx]\n",
    "\n",
    "    X_test =  X[split_idx:]\n",
    "    y_test =  y[split_idx:]\n",
    "\n",
    "\n",
    "    print(f\"Training set: {len(X_train)} samples\")\n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "    #Create and train model\n",
    "    model = SimpleLinearRegression(learning_rate = 0.01, iterations = 1000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    #Make Predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    print(\"First 5 test predictions vs actual:\")\n",
    "    for i in range(min(5, len(y_test))):\n",
    "        print(f\" x={X_test[i]:.2f} -> Predicted: {y_pred_test[i]:.2f}, \"\n",
    "              f\" Actual: {y_test[i]:.2f}, Error: {abs(y_pred_test[i] - y_test[i]):.2f}\")\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f68c70-3e5c-40be-912d-49760efb5a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
