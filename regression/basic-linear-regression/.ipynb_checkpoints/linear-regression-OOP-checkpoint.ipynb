{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c76ee19f-dab9-40ea-b537-768ea41c0959",
   "metadata": {},
   "source": [
    "## Simple Linear Regression From Scratch \n",
    "\n",
    "#### Goal: Find the best straight line through data points\n",
    "#### Formula: y = wx + b\n",
    "\n",
    "- Where:\n",
    "  - w = slope (how steeo the line is)\n",
    "  - b = y - intercept (where line crosses y-axis)\n",
    "  - x = input (independent variable)\n",
    "  - y = output (dependent variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895e0fc6-94a3-463e-9c8f-778c471cd9e3",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "### OOP Concepts Covered:\n",
    "1. Class - Blueprint for creating regression models\n",
    "2. Constructor `(__init__)` - Initialize model when created\n",
    "3. Instance Variables (self.w, self.b) - Data stored in each model\n",
    "4. Methods (fit, predict, evaluate) - Actions the model can perform\n",
    "5. Encapsulation - Bundling data and methods together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e736c4b-eb23-4f28-8d2c-b33a780048ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21cb34fc-50c6-46f5-8f41-7858a6101008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearRegression():\n",
    "    \"\"\"\n",
    "    This class represents ONE linear regression model.\n",
    "    Each model learns its own slope(w) and intercept(b).\n",
    "    \"\"\"\n",
    "\n",
    "    #CONSTRUCTOR: Initialize a new model\n",
    "    def __init__(self, learning_rate = 0.01, iterations = 1000):\n",
    "        \"\"\"\n",
    "        1. python creates a new object in memeory.\n",
    "        2. __init__ is called automatically.\n",
    "        3. self.lr gets set to 0.01\n",
    "        4. self.iterations gets set to 1000\n",
    "        5. Other attributes initialized to staring values.\n",
    "        \"\"\"\n",
    "\n",
    "        #Instance Variable (Attributes)\n",
    "        #These belong to THIS specific model instance.\n",
    "        #Different models can have different values.\n",
    "\n",
    "        #Store hyperparameters (settings we choose)\n",
    "        self.lr = learning_rate\n",
    "        self.iterations = iterations\n",
    "\n",
    "        #Model parameters (What the model LEARNS)\n",
    "        self.w = 0.0\n",
    "        self.b = 0.0\n",
    "\n",
    "        #Training history (for visualization) \n",
    "        self.cost_history = [] #Track cost at each iteration\n",
    "\n",
    "        #Status Flag\n",
    "        self.is_trained = False  #Has the model been trained yet?\n",
    "\n",
    "\n",
    "        print(f\"Learning rate: {self.lr}\")\n",
    "        print(f\"Iterations: {self.iterations}\")\n",
    "        print(f\"Initial Slope(w): {self.w}\")\n",
    "        print(f\"Intial Intercept(b): {self.b}\")\n",
    "        print()\n",
    "\n",
    "    #METHOD: fit() -----> Train the model (learn m and b)\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model to find the best w and b\n",
    "\n",
    "        Parameters:\n",
    "        X : list\n",
    "            Input Values (Independent variables)\n",
    "            eg: [1, 2, 3, 4, 5]\n",
    "\n",
    "        y: list\n",
    "            Output Values (dependent variables)\n",
    "            eg:[90, 80, 30, 40,  20]\n",
    "\n",
    "        Gradient Descent Steps:\n",
    "        1. Start with random/zero values for w and b\n",
    "        2. Make predictions with current w and b\n",
    "        3. Calculate how wrong we are (cost function)\n",
    "        4. Calculate gradients (which direction to adjust w and b)\n",
    "        5. Update w and b in that direction\n",
    "        6. Repeat steps many times\n",
    "        7. Eventually converge to best w and b\n",
    "        \"\"\"\n",
    "\n",
    "        #Number of training examples\n",
    "        n = len(X)\n",
    "        print(f\"Number of training examples: {n}\\n\")\n",
    "        print(f\"{'Iter':<10} {'Cost':<15} {'m':<15} {'b':<15}\")\n",
    "\n",
    "\n",
    "\n",
    "        #Gradient Descent loop\n",
    "        for iterations in range(iterations):\n",
    "            predictions = []\n",
    "            for i in range(n):\n",
    "                y_pred = self.w * X[i] + self.b\n",
    "                predictions.append(y_pred)\n",
    "\n",
    "\n",
    "            #Calculate Cost (Mean Squared Error)\n",
    "            #Cost Measures how bad our predictions are\n",
    "            #Lower cost --> better predictions\n",
    "            #Formula: Cost = (1/n) *  Σ(y_pred - y_actual)²\n",
    "\n",
    "            total_error = 0.0\n",
    "            for i in range(n):\n",
    "                error = predictions[i] - y[i]\n",
    "                squared_error = error ** 2\n",
    "                total_error += squared_error\n",
    "            cost = total_error /n \n",
    "\n",
    "            self.cost_history.append(cost)\n",
    "\n",
    "\n",
    "            #Calculate Gradients \n",
    "            #Gradients tell us HOW to change w and b to reduce cost\n",
    "            # These formulas come from calculus (partial derivatives):\n",
    "            # ∂Cost/∂m = (2/n) * Σ(y_pred - y_actual) * x\n",
    "            # ∂Cost/∂b = (2/n) * Σ(y_pred - y_actual)\n",
    "\n",
    "            #Gradient for w (slope)\n",
    "            gradient_w = 0.0\n",
    "            for i in range(n):\n",
    "                error = predictions[i] - y[i]\n",
    "                gradient_w += error * X[i]\n",
    "            gradient_w = (2/n) * gradient_w\n",
    "\n",
    "            gradient_b = 0.0\n",
    "            for i in range(n):\n",
    "                error = prediction[i] - y[i]\n",
    "                gradient_b += error\n",
    "            gradient_b = (2/n) * gradient_b\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f68c70-3e5c-40be-912d-49760efb5a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
